{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b08289",
   "metadata": {},
   "source": [
    "# 1.\n",
    "- 목표 : 단순하게 잘못된 분류 결과의 숫자를 가능한 한 줄이는 것\n",
    "- x를 클래스 중 하나에 포함시키는 규칙이 필요하다.\n",
    "- 이 규칙은 입력 공간을 DECISION REGION이라는 결정 구역 $R_K$들로 분리한다.\n",
    "- 이 구역 사이들의 경계를 결정 경계 or 결정 표면 이라고 한다.\n",
    "- 항상 이게 정답으로 착착 분류 될 가능성은 희박하므로 이거에 관한 식을 소개하겠다.\n",
    "    - $p(mistake) = p(x \\in R_1, C_2) + p(x \\in R_2, C_1) = \\int_{R_1} p(x,C_2)dx + \\int{R_2}p(x,C_1)dx$\n",
    "    - 그냥 간단하게 오분류 될 확률을 더해준거고, 맨 오른쪽 식은 오분류 될 확률 분포를 각각 더해준것이다.\n",
    "- 앞으로 우리는 이것을 최소화하는 방향으로 모델을 설계해야한다.\n",
    "    - <img src = \"http://norman3.github.io/prml/images/Figure1.24.png\" width = 300 height = 300>\n",
    "    - 그림을 뜯어보자\n",
    "    - $\\widehat{x}$ 보다 크면 $C_2, R_2$에 분류 되고 작으면 $C_1, R_1$에 분류 된다.  \n",
    "    - $C_1$이 오분류 된것은 보라색 부분, $C_2$이 오분류 되면 빨 + 녹색 부분이다.\n",
    "    - $\\widehat{x}$가 변하게 되면 오류 부분의 합은 똑같지만 빨간색 구역의 크기만 변할 것이다.\n",
    "    - 다음 그림에서 최적의 결정 경계는 두 분포가 만나는 지점$x_0$에 해당한다.\n",
    "    - 왜냐면 여기서는 빨간 부분이 아예 없어지게 된다. 이러면 면적이 확 줄어든다.\n",
    "- 반대로 K개의 클래스를 가진 경우에는 올바르게 분류된 경우의 확률을 극대화하는 것이 더 쉽다.\n",
    "    - $p(correct) = \\sum_{k=1}^K p(x \\in R_k, C_k) = \\sum_{k=1}^K \\int_{R_k} p(x, C_k)dx$ 이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
