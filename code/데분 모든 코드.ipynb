{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#저장방법\n",
    "df.to_csv('voc_rcp_practice2.csv', index=False)\n",
    "# 첫번째 컬럼 \"voc_trt_perd_itg_cd\" 데이터 확인 및 값 분포 확인\n",
    "df['voc_trt_perd_itg_cd'].value_counts()\n",
    "# 첫번째 컬럼 \"voc_trt_perd_itg_cd\" 값 분포 비율 확인\n",
    "df['voc_trt_perd_itg_cd'].value_counts(normalize=True)\n",
    "# '_' 데이터 비율이 54% 차지하는 것으로 확인됨으로 해당 컬럼은 학습 데이터로 적합하지 않음으로 삭제\n",
    "df = df.drop(columns=['voc_trt_perd_itg_cd'])\n",
    "df.drop(columns=['new_date', 'opn_nfl_chg_date', 'cont_fns_pam_date'], inplace=True)\n",
    "#\"_\" 항목을 NaN 값으로 치환\n",
    "df['age_itg_cd'] = df['age_itg_cd'].replace(\"_\", np.NaN)\n",
    "df=df.astype({'age_itg_cd': int})\n",
    "df['age_itg_cd'].isnull().sum()\n",
    "# 'age_itg_cd' 항목의 type 변경\n",
    "# NaN의 경우 int type을 지원하지 않아 float type으로 변경\n",
    "df=df.astype({'age_itg_cd': float})\n",
    "#info 로 확인\n",
    "# 40% 이상 이상한 데이터 있는거 찾기\n",
    "[df[c].value_counts(normalize=True) for c in df]\n",
    "# 모든 _ 싹 바꿔버리는 코드\n",
    "df_temp = df.replace('_', np.nan)\n",
    "df_temp.isnull().sum()\n",
    "# 싹 지워버리는 코드\n",
    "df.drop(columns=['voc_trt_reslt_itg_cd',\n",
    "                'oos_cause_type_itg_cd',\n",
    "                'engt_cperd_type_itg_cd',\n",
    "                'engt_tgt_div_itg_cd',\n",
    "                'fclt_oos_yn',\n",
    "                ], inplace=True)\n",
    "# info 보고 형식 바꾸는 방법\n",
    "df = df.astype({'voc_prod_sbt_id': object,\n",
    "               'voc_wjt_sorc_id':object,\n",
    "               'voc_type_itg_cd':object,\n",
    "               'voc_sttus_itg_cd':object,\n",
    "               'bprod_sbt_id':object,\n",
    "               'voc_trt_degr_div_itg_cd':object,\n",
    "               'voc_trt_need_time_itg_cd':object})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer ( 대신 가변수화 먼저 진행하고 해야함)\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed = imputer.fit_transform(x)\n",
    "# 데이터프레임으로 다시 만듭시다.\n",
    "x = pd.DataFrame(imputed, columns=x.columns)\n",
    "x.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df Data 복사\n",
    "df_fix = df.copy()\n",
    "#fillna 함수를 사용해서 특정 숫자나 문자로 결측치를 처리하는 방법\n",
    "df=df.fillna(15)\n",
    "df=df.fillna(method='backfill') \n",
    "df=df.fillna(method='ffill')\n",
    "df['age_itg_cd']=df['age_itg_cd'].replace(np.nan, df['age_itg_cd'].median())\n",
    "df=df.dropna() #listwise 방식으로 제거하기 record의 항목 중 1개의 값이라도 NA이면 해당 데이터 행 전체 제거\n",
    "df=df.dropna(how='all') #pairwise 방식으로 제거하기 모든 항목이 NA인 데이터 행만 제거\n",
    "df=df.dropna(thresh=14) # 14개이상이면 제거\n",
    "df=df.dropna(subset=['cust_clas_itg_cd'])  # 특정열에 있는 na 만제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이상치제거\n",
    "df_data=df[(df['cust_clas_itg_cd']!='R')]\n",
    "# 이상치변경\n",
    "df_data['cust_clas_itg_cd']=df_data['cust_clas_itg_cd'].replace('R','L')\n",
    "# 보통 value_counts()로 최빈값을 뽑아서 R을 L로 바꾸는 형식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "# 박스플롯\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.boxplot(df['age_itg_cd'])\n",
    "plt.show()\n",
    "# 이상치 제거\n",
    "#Q1, Q3 구하기\n",
    "q1 = df['age_itg_cd'].quantile(0.25)\n",
    "q3 = df['age_itg_cd'].quantile(0.75)\n",
    "# 1.5 * IQR(Q3 - Q1)\n",
    "iqr = 1.5 * (q3 - q1) \n",
    "# 이상치 제거하기\n",
    "df_data=df[(df['age_itg_cd'] < (q3 + iqr)) & (df['age_itg_cd'] > (q1 - iqr))]\n",
    "# 이상치 변경하기(최대 최소)\n",
    "age_min = q1 - iqr\n",
    "age_max = q3 + iqr\n",
    "# 이상치를 변경\n",
    "df[(df['age_itg_cd'] > age_max)]= age_max\n",
    "df[(df['age_itg_cd'] < age_min)]= age_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최빈값 찾기\n",
    "print(df['cont_sttus_itg_cd'].value_counts())\n",
    "print(df['cust_dtl_ctg_itg_cd'].value_counts())\n",
    "print(df['voc_mis_pbls_yn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화\n",
    "# Seaborn을 사용하여 간단한 차트를 그리기\n",
    "plt.figure()\n",
    "sns.lineplot([1,2,3], [100,120,110])\n",
    "plt.show()\n",
    "# scatter\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y='cust_snsry_base_conf_need_time', x='voc_trt_need_time_itg_cd', data=df)\n",
    "plt.show()\n",
    "# [Hint] 빈도는 파라미터 bins 활용 20개\n",
    "plt.figure()\n",
    "sns.histplot(x='age_itg_cd', bins=20 , data= df)\n",
    "plt.xlabel('나이')\n",
    "plt.ylabel('고객 수')\n",
    "plt.show()\n",
    "#나이대별 총이용금액 분포를 박스 그래프로 그리기\n",
    "plt.boxplot(df['age_itg_cd'])\n",
    "plt.show()\n",
    "#컬럼별 상과관계를 heatmap 그래프로 그리기\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()\n",
    "#두개선을 가지는 임의의 선그래프를 그리고 범례 추가하기\n",
    "plt.plot([1,2,3], [1,4,9])\n",
    "plt.plot([2,3,4],[5,6,7])\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Game Result')\n",
    "plt.legend(['A team', 'B team'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'age_itg_cd'를 활describe 나이대(\"by_age\")  Feature 만들기  (그냥 50대, 40대 칸 추가)\n",
    "df['by_age']=df['age_itg_cd']//10*10\n",
    "df=df.astype({'age_itg_cd': int, 'by_age':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder() 호출\n",
    "le = LabelEncoder()\n",
    "df['le_cust_clas_itg_cd'] = le.fit_transform(df['cust_clas_itg_cd'])\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = ['voc_type_itg_cd',\n",
    "                  'voc_sttus_itg_cd',\n",
    "                 'cust_clas_itg_cd',\n",
    "                 'cont_sttus_itg_cd',\n",
    "                 'cust_dtl_ctg_itg_cd',\n",
    "                 'voc_trt_degr_div_itg_cd',\n",
    "                 'voc_trt_need_time_itg_cd']\n",
    "df = pd.get_dummies(df, columns=object_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분리\n",
    "X = df.drop(columns=['trm_yn'])\n",
    "y = df['trm_yn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "x_test_std = scaler.transform(X_test)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 별 accuracy, precission , recall, f1_score를 List 로 저장\n",
    "perfomance_acc = [accuracy_score(y_test, lr_pred), accuracy_score(y_test, knn_pred), accuracy_score(y_test, dt_pred), accuracy_score(y_test, rfc_pred)]\n",
    "perfomance_precision = [precision_score(y_test, lr_pred), precision_score(y_test, knn_pred), precision_score(y_test, dt_pred), precision_score(y_test, rfc_pred)]\n",
    "perfomance_recall = [recall_score(y_test, lr_pred), recall_score(y_test, knn_pred), recall_score(y_test, dt_pred), recall_score(y_test, rfc_pred)]\n",
    "perfomance_f1_score = [f1_score(y_test, lr_pred), f1_score(y_test, knn_pred), f1_score(y_test, dt_pred), f1_score(y_test, rfc_pred)]\n",
    "performance_index = ['accuary', 'precission', 'recall', 'f1_score']\n",
    "performance_columns = ['Logistic', 'KNN', 'Decision', 'RandomForest']\n",
    "# 모델 별 전체 성능을 데이터프레임으로 저장ㅌ\n",
    "performance_total= pd.DataFrame(data=[perfomance_acc, perfomance_precision, perfomance_recall, perfomance_f1_score], index=performance_index, columns=performance_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델당 정확도\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title('Accuracy')\n",
    "sns.barplot(x=perfomance_acc, y=performance_columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc 중요도\n",
    "feature = df.drop(columns=['trm_yn'])\n",
    "rfc_importances_values = rfc_model.feature_importances_\n",
    "rfc_importances = pd.Series(rfc_importances_values, index=feature.columns )\n",
    "rfc_top10 = rfc_importances.sort_values(ascending=False)[:10]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "sns.barplot(x=rfc_top10, y = rfc_top10.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "model = KNeighborsClassifier(n_neighbors=2)\n",
    "cv_score = cross_val_score(model, x_train_s, y_train, cv= 10)\n",
    "result = {}\n",
    "result['KNN'] = cv_score.mean()\n",
    "#DecisionTree\n",
    "model = DecisionTreeClassifier(random_state=2022)\n",
    "cv_score = cross_val_score(model, x_train, y_train, cv=10)\n",
    "result['Decision Tree'] = cv_score.mean()\n",
    "#LogisticRegression\n",
    "model = LogisticRegression(max_iter=500)\n",
    "cv_score = cross_val_score(model, x_train_s, y_train, cv=10)\n",
    "result['Logistic Regression'] = cv_score.mean()\n",
    "# SVM\n",
    "model = SVC(random_state=2022)\n",
    "cv_score = cross_val_score(model, x_train_s, y_train, cv=10)\n",
    "result['SVM'] = cv_score.mean()\n",
    "# RandomForest\n",
    "model = RandomForestClassifier(random_state=2022)\n",
    "cv_score = cross_val_score(model, x_train, y_train, cv=10)\n",
    "result['Random Forest'] = cv_score.mean()\n",
    "#XGB\n",
    "model = XGBClassifier(eval_metric='mlogloss',random_state=2022)\n",
    "cv_score = cross_val_score(model, x_train, y_train, cv=10)\n",
    "result['XGBoost'] = cv_score.mean()\n",
    "# 평가 결과 확인\n",
    "for m_name, m_score in result.items():\n",
    "    print('='*56)\n",
    "    print(m_name)\n",
    "    print('-'*56)\n",
    "    print(m_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선언\n",
    "#model_svm = SVC()\n",
    "\n",
    "# 파라미터 선언\n",
    "#param = {'C':[0.01,1,10],\n",
    "         #'gamma': [0.01,0.1,1]}\n",
    "\n",
    "# Grid Search 모델 선언\n",
    "#model_svc = GridSearchCV(model_svm,\n",
    "                         #param,\n",
    "                         #cv=10,\n",
    "                         #scoring='accuracy')\n",
    "            #model_svc.fit(x_train_s, y_train)\n",
    "            \n",
    "# 최적파라미터, 성능 확인\n",
    "#print(model_svc.best_params_)\n",
    "#print(model_svc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(eval_metric='mlogloss', random_state=2022)\n",
    "param = {'max_depth':range(4,7),\n",
    "         'n_estimators':range(60,81,10)}\n",
    "model_xgb = GridSearchCV(model, param, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d-tree\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(25,10))\n",
    "plot_tree(model_dt.best_estimator_,\n",
    "         filled=True,\n",
    "         feature_names=list(x),\n",
    "         fontsize=8,\n",
    "         class_names=['No', 'Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(32, activation='relu', input_shape=(81,)))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es\n",
    "# monitor : EalryStopping의 기준이 되는 값('val_loss'를 입력하면 val_loss가 더 이상 감소되지 않을 경우 적용)\n",
    "# min_delta : 개선된 것으로 간주하기 위한 최소한의 변화량\n",
    "# patience : monitor되는 값의 개선이 없을 때 최적의 값을 기준으로 몇 번의 epoch를 더 진행할 지 정하는 값\n",
    "# verbose : 화면에 상태 표시\n",
    "# mode : monitor가 최소가 되어야 하는 지, 최대가 되어야 하는지 ('val_accuracy'는 max, 'val_loss'는 min)\n",
    "\n",
    "# mc\n",
    "# filepath : 모델을 저장할 경로를 입력합니다. '파일명.h5'\n",
    "# 모델을 저장할 때, 기준이 되는 값을 지정합니다. ('val_loss'를 입력하면 validation set의 loss가 가장 적을 때 저장)\n",
    "# verbose : 화면에 상태 표시\n",
    "# save_best_only : True, False\n",
    "# mode : auto로 할 경우, 모델이 알아서 min, max 판단하여 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "mc = ModelCheckpoint('my_checkpoint.h5', monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=20, \n",
    "          callbacks=[es, mc],\n",
    "          batch_size=16,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(model2.history.history)\n",
    "plt.plot(performance[['loss','val_loss']])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(performance[['loss','val_loss', 'accuracy','val_accuracy']])\n",
    "plt.legend(['loss','val_loss', 'accuracy','val_accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_q = pd.DataFrame(model_q.history.history)\n",
    "\n",
    "plt.plot(performance_q[['accuracy', 'val_accuracy']])\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
