{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5e4924",
   "metadata": {},
   "source": [
    "# [딥러닝]역전파 - Back Propagation\n",
    "\n",
    "# 0.\n",
    "\n",
    "학교에서 역전파 알고리즘을 배우고 손으로 직접 계산까지해보고 과제풀이도 하고 시험에도 출제되었다.\n",
    "\n",
    "다층 퍼셉트론에 대해 간략히 쓰다가 역전파에 대해서 간단하게 설명이 막혀버려서 ... 이렇게 글을 쓴다. 생각보다 마무리 부분을 더 잘 쓴거 같다. 그냥 계산 과정 보기 싫으면 마무리만 보면 핵심만 요약 했다.\n",
    "\n",
    "너무 복잡하고 내가 직접한 것은 올리긴했지만 그래도 컴터로 작업하는건 너무 힘들어서 참고좀 했다... 감사합니다...어떤 싸움을 해오신겁니까\n",
    "\n",
    "출처 : [https://wikidocs.net/37406](https://wikidocs.net/37406)\n",
    "\n",
    "1. Feed Forward(순전파)\n",
    "\n",
    " \n",
    "\n",
    "![https://wikidocs.net/images/page/37406/backpropagation_2.PNG](https://wikidocs.net/images/page/37406/backpropagation_2.PNG)\n",
    "\n",
    "다음 그림에서 순전파를 진행하겠다.\n",
    "\n",
    "처음에는 가중치 값을 랜덤하게 준다.\n",
    "\n",
    "각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉충 뉴런의 시그모이드 함수의 입력값이 된다.\n",
    "\n",
    "식으로 보는게 더 이해가 빠르다.\n",
    "\n",
    "$z_1 = w_1x_1 + w_2x_2$ = 0.3 * 0.1 + 0.25 * 0.2 = 0.08\n",
    "\n",
    "$z_2 = w_3x_1 + w_4x_2$ = 0.4 * 0.1 + 0.35 * 0.2 = 0.11\n",
    "\n",
    "이 다음으로 $h_1, h_2$를 구해야 하는데 이건 활성화함수고 시그모이드라고 하겠다.\n",
    "\n",
    "$h_1 = sigmoid(z_1) = 0.51998934$\n",
    "\n",
    "$h_2 = sigmoid(z_2) = 0.52747230$\n",
    "\n",
    "(시그모이드 계산법이 더러우니 생략한다. ㅋㅋ.. relu 쓰면 개편하긴한데. )\n",
    "\n",
    "그다음 h1, h2 를 아니... 또 $z_3,z_4$ 를 구해보자\n",
    "\n",
    "$z_3 = w_5h_1 + w_6h_2 = 0.45h_1 + 0.4h_2 =0.44498412$\n",
    "\n",
    "$z_4 = w_7h_1 + w_8h_2 = 0.7h_1 + 0.6h_2 = 0.68047592$\n",
    "\n",
    "이제 다시 시그모이드함수에 저 값을 넣어주면 된다.\n",
    "\n",
    "$\\sigma_1 = sigmoid(z_3) = 0.60944600$\n",
    "\n",
    "$\\sigma_2 = sigmoid(z_4) = 0.66384491$\n",
    "\n",
    " \n",
    "\n",
    "마지막으로 해야할 일은 예측값과 실제값의 오차를 계산하는 오차 함수를 선택해야한다.\n",
    "\n",
    "보통 MSE를 사용하니까 우리도 여기서 MSE를 사용하자\n",
    "\n",
    "$E_1 = \\frac{1}{2}$(실제값1 - 예측값1)^2 = 0.02193381\n",
    "\n",
    "$E_2 = \\frac{1}{2}$(실제값2 - 예측값2)^2 = 0.00203809\n",
    "\n",
    "$E = E_1 + E_2 = 0.024$\n",
    "\n",
    "이렇게 오차가 0.024임을 알 수 있다.\n",
    "\n",
    "# 2. 역전파 1단계\n",
    "\n",
    "순전파가 앞에서 부터 저렇게 값을 줬다면 역전파는 출력된 값에서 입력 으로 가면서 가중치를 업데이트해간다. \n",
    "\n",
    "1단계가 뭐냐면 이게 조금 복잡하기 때문에 출력에서 가운데로 가는걸 1단계 가운데에서 입력으로 가는걸 2단계로 하겠다.\n",
    "\n",
    "![https://wikidocs.net/images/page/37406/backpropagation_3.PNG](https://wikidocs.net/images/page/37406/backpropagation_3.PNG)\n",
    "\n",
    "자 이걸 이용하자.\n",
    "\n",
    "여기서 업데이트 해야 할 가중치는 $w_5,w_6,w_7,w_8$ 이다.\n",
    "\n",
    "이걸 업데이트 하기위해선 gradient descent와 Chain Rule을 이용해야한다.\n",
    "\n",
    "![https://github.com/dhy02094/STUDY_TH/blob/master/확률과정론/스크린샷%202022-01-03%20오전%202.23.12.png?raw=true](https://github.com/dhy02094/STUDY_TH/blob/master/확률과정론/스크린샷%202022-01-03%20오전%202.23.12.png?raw=true)\n",
    "\n",
    "다음 식은 L 은 오차함수를 미분하고, h는 활성화 함수를 미분하고, y는 위 그림에서 보면 z를 의미하며 순전파과정에서 쓴 식을 미분해서 사용한다.\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial \\sigma_1}\\frac{\\partial \\sigma_1}{\\partial z_3}\\frac{\\partial z_3}{\\partial w_5}$$\n",
    "\n",
    "그냥 다 하자\n",
    "\n",
    "$$E_{total}= \\frac{1}{2}(true_1 - pred_1)^2 + \\frac{1}{2}(true_2 - pred_2)^2$$\n",
    "\n",
    "이걸 이제 미분해줄거다 시그마에대해...\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial \\sigma_1}= -(true_1 - pred_1) = -(0.4 - 0.60944600) = 0.20944600$$\n",
    "\n",
    "시그모이드의 미분은 \n",
    "\n",
    "$$\\sigma_1(1 - \\sigma_1) = 0.60944600*(1-0.60944600) = 023802157$$\n",
    "\n",
    "$z_3$의 $w_5$에 대한 미분은 그냥 $h_1$ 이다. 위에 보면 일차식이기 때문이다.\n",
    "\n",
    "$h_1 = 0.51998934$\n",
    "\n",
    "이제 이걸 맨 위에 식에 다 곱해주면 0.02592286이 나온다. \n",
    "\n",
    "이제 경사하강법을 통해 가중치를 업데이트 해주자. \n",
    "\n",
    "하이퍼파라미터에 해당하는 학습률은 0.5라고하자.\n",
    "\n",
    "$$w_5^+=w_5-\\alpha \\frac{\\partial E_{total}}{\\partial w_5} = 0.45 -0.5*0.02592286 = 0.43703857$$\n",
    "\n",
    "$$w_6^+ = 0.387, w_7^+=0.7, w_8^+ =0.6 $$\n",
    "\n",
    "# 3. 역전파 2단계\n",
    "\n",
    "![https://wikidocs.net/images/page/37406/backpropagation_4.PNG](https://wikidocs.net/images/page/37406/backpropagation_4.PNG)\n",
    "\n",
    "아까 한 과정을 똑같이 옆에 해주면 된다. \n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial h_1}\\frac{\\partial h_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}$$\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial h_1}  = \\frac{\\partial E_{1}}{\\partial h_1} + \\frac{\\partial E_{2}}{\\partial h_1} $$\n",
    "\n",
    "$$\\frac{\\partial E_{1}}{\\partial h_1}=\\frac{\\partial E_{1}}{\\partial z_3}\\frac{\\partial z_3}{\\partial h_1} = \\frac{\\partial E_{1}}{\\partial \\sigma_1} \\frac{\\partial \\sigma_1}{\\partial z_3}\\frac{\\partial z_3}{\\partial h_1} = -(true_1 -pred_1)\\sigma_1(1-\\sigma_1)w_5=0.02    $$\n",
    "\n",
    "$$\\frac{\\partial E_{2}}{\\partial h_1}=\\frac{\\partial E_{2}}{\\partial z_4}\\frac{\\partial z_4}{\\partial h_1} = \\frac{\\partial E_{2}}{\\partial \\sigma_2} \\frac{\\partial \\sigma_2}{\\partial z_4}\\frac{\\partial z_4}{\\partial h_1} = -(true_2 -pred_2)\\sigma_2(1-\\sigma_2)w_5=0.01 $$\n",
    "\n",
    "$$\\frac{\\partial z_1}{\\partial w_1}=x_1 = 0.1$$\n",
    "\n",
    "$$\\frac{\\partial h_1}{\\partial z_1}=h_1(1-h_1) = 0.25$$\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial w_1} = 0.001$$\n",
    "\n",
    "$$w_1^+ = w_1 -\\alpha\\frac{\\partial E_{total}}{\\partial w_1} = 0.1 - 0.5*0.001 = 0.29959556$$\n",
    "\n",
    "(반올림때려서 눈으로봐도 값이 다르지만 그냥 그런가보다.. 하자..)\n",
    "\n",
    "나머지 $w_2^+=0.25, w_3^+=0.4, w_4^+=0.35$ 로 계산 가능하다\n",
    "\n",
    "# 4. 마무리\n",
    "\n",
    "이 고생을 왜했는가? \n",
    "\n",
    "자 우리는 입력을 넣었을 때 우리가 알고 있는 정답인 출력이 나오기를 기대한다. 근데 그게 쉽지가 않다. 그러기 위해서 우리는 다음 다층 퍼셉트론을 이용해서 그 정답과 아주 유사하게 나오는 학습파라미터 즉 w, 가중치를 찾아주고 싶은거다. \n",
    "\n",
    "당연히 우리는 w에 대해서 모른다. 뭘 넣어야 저런 출력이 나오는지 직관적으로 이해 할 수  없다. \n",
    "\n",
    "그래서 나온 방법이 위 방법이다. 요약하면 \n",
    "\n",
    "1. 일단 w에 랜덤한 아무 숫자나 넣은다.( 좋고 적합한 수를 넣으면 계산량이 줄겟지?) \n",
    "2. 그 다음 feed forward 로 위에서 정한 w가 얼마나 정확한지 MSE오차함수를 이용해 판단한다.\n",
    "3. 그리고 이제 w를 업데이트 해야하므로 출력에서 입력쪽으로 그래디언트 디센트를 이용해서 미분값을 구해서 학습률을 업데이트 해준다.\n",
    "4. 이렇게 되면 우리는 새로운 w를 갖게 된다. \n",
    "5. 위에서는 안했지만 이런 새로운 w로 feed forward 하면 오차가 감소한 것을 확인할 수 있다.\n",
    "6. 이게 바로 1Epoch 한거다. 우리는 이걸 100번,1000번, 10000번 하니까 인간이 절대 컴터를 못이기는거 같다.\n",
    "\n",
    "다음은 내가 과제로 직접 눈물흘리며 손으로 푼건다.\n",
    "\n",
    "[https://github.com/dhy02094/STUDY_TH/blob/master/확률과정론/역전파 계산.pdf](https://github.com/dhy02094/STUDY_TH/blob/master/%ED%99%95%EB%A5%A0%EA%B3%BC%EC%A0%95%EB%A1%A0/%EC%97%AD%EC%A0%84%ED%8C%8C%20%EA%B3%84%EC%82%B0.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
